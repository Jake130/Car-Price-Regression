"""
Model Definition
Basic NN, revision 2

by Jake Kolster, Alex Jo√£o Peterson Santos
March 13, 2025
for CS 453 Project

Defines the model architecture, dataset, and dataloader for the basic neural net model.

HEADS UP
Do not edit this file to revise the model! Create a new model dir instead!
This makes it clear which model architecture training/test records originated from when doing data analysis.
"""

import os
import pickle
import torch
from torch import nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader


class Argument:
    """
    Specifies hyperparameters needed the training of this model.

    The input list matches the following model-specific hyperparameters:
    0: Dataset directory
    1: Depth, or number of layers
    2: Width of each layer
    3: Batch size
    4: Learning rate
    5: SGD momentum (-1 for AdamW optimizer instead)
    6: Activation function ("sigmoid", "relu", "leaky_relu")
    7: Gradient clipping norm (0 for no clipping)
    8: Learning rate scheduling gamma (0 for no scheduling)
    """

    def __init__(self, values: list):
        # make sure correct number hyperparameters passed
        if len(values) < 9:
            raise ValueError("Not enough hyperparameters passed to model")
        elif len(values) > 9:
            raise ValueError("Too many hyperparameters passed to model")

        # valid dataset directory
        if not os.path.isdir(values[0]):
            raise ValueError(f"Invalid dataset directory: {values[0]}")
        self.dataset_dir = values[0]

        # valid depth (num layers)
        values[1] = int(values[1])
        if values[1] % 1 != 0 or values[1] < 1:
            raise ValueError(f"Invalid depth: {values[1]}")
        self.depth = int(values[1])

        # valid width (num neurons per layer)
        values[2] = int(values[2])
        if values[2] % 1 != 0 or values[2] < 1:
                    raise ValueError(f"Invalid width: {values[2]}")
        self.width = int(values[2])

        # valid batch size
        values[3] = int(values[3])
        if values[3] % 1 != 0 or values[3] < 1:
            raise ValueError(f"Invalid batch size: {values[3]}")
        self.batch_size = int(values[3])

        # valid learning rate
        values[4] = float(values[4])
        if values[4] <= 0:
            raise ValueError(f"Invalid learning rate: {values[4]}")
        self.learning_rate = float(values[4])

        # valid SGD momentum (-1 is a special reserved value)
        values[5] = float(values[5])
        if values[5] != -1 and (values[5] <= 0 or values[5] >= 1):
            raise ValueError(f"Invalid SGD momentum: {values[5]}")
        self.use_sgd = values[5] != -1
        self.sgd_momentum = float(values[5])

        # valid activation function
        values[6] = values[6].lower()
        if values[6] not in ["sigmoid", "relu", "leaky_relu"]:
            raise ValueError(f"Invalid activation function: {values[6]}")
        self.activation_fn = values[6]
    
        # valid gradient clipping norm
        values[7] = float(values[7])
        if values[7] < 0:
            raise ValueError(f"Invalid clipping norm: {values[7]}")
        self.gradient_clipping_norm = values[7]

        # valid learning rate schedule
        values[8] = float(values[8])
        if values[8] < 0:
            raise ValueError(f"Invalid learning rate schedule: {values[8]}")
        self.lr_schedule = values[8]

        # create model name
        self.generic_name = "basic_nn2"
        self.name = "{}_{}x{}_bs{}_lr{}_sg{}_af{}_gc{}_ls{}".format(
            self.generic_name,
            self.depth,
            self.width,
            self.batch_size,
            self.learning_rate,
            "-1" if not self.use_sgd else self.sgd_momentum,
            {"sigmoid": "0", "relu": "1", "leaky_relu": "2"}[self.activation_fn],
            self.gradient_clipping_norm,
            self.lr_schedule
        )

    def __str__(self):
        return f"""Arguments and hyperparameters:
Model name: {self.name}
Dataset directory: {self.dataset_dir}
Depth: {self.depth}
Width: {self.width}
Batch size: {self.batch_size}
Learning rate: {self.learning_rate}
Use SGD: {self.use_sgd}
SGD momentum: {"N/A" if not self.use_sgd else self.sgd_momentum}
Activation function: {self.activation_fn}
Gradient clipping norm: {self.gradient_clipping_norm}
Learning rate schedule: {self.lr_schedule}
"""

    def hyperparameter_dict(self):
        return {
            "Depth": self.depth,
            "Width": self.width,
            "Batch size": self.batch_size,
            "Learning rate": self.learning_rate,
            "Use SGD": self.use_sgd,
            "SGD momentum": "N/A" if not self.use_sgd else self.sgd_momentum,
            "Activation function": self.activation_fn,
            "Gradient clipping norm": self.gradient_clipping_norm,
            "Learning rate schedule": self.lr_schedule
        }


class Model(nn.Module):
    """
    Definition of the layers of the neural network.
    """

    def __init__(self, args):
        super().__init__()
        
        # activation function based on config
        activation_fn = {
            "sigmoid": torch.nn.Sigmoid(),
            "leaky_relu": torch.nn.LeakyReLU(negative_slope=0.01),
            "relu": torch.nn.ReLU()
        }[args.activation_fn]

        assert isinstance(args.depth, int)
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(48, args.width),  # 48 features
            *[
                activation_fn,
                torch.nn.Linear(args.width, args.width)
            ] * args.depth,  # Hidden layers repeated 'depth' times
            torch.nn.Linear(args.width, 1)  # Output layer: width -> 1
        )

    def forward(self, x):
        return self.layers(x)


def create_transform():
    """
    Create and return the transform for the model.
    """
    # no transformations are needed here
    transform = transforms.Compose([])
    return transform


def create_model(args: Argument):
    """
    Create and return the model.
    """
    return Model(args)


# loss function definition
def create_loss_function(args: Argument):
    """
    Create and return the loss function for the model.
    """
    return nn.MSELoss()


# optimizer definition
def create_optimizer(args: Argument, model: Model):
    """
    Create and return the optimizer for the model.
    """
    if args.use_sgd:
        print("Ran create_optimizer with SGD")
        return torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.sgd_momentum)
    else:
        print("Ran create_optimizer with AdamW")
        return torch.optim.AdamW(model.parameters(), lr=args.learning_rate)


class CarPriceDataset(Dataset):
    """
    Define the dataset for the car price data.
    """

    def __init__(self, args: Argument, type):
        assert type in ["train", "test", "val"]
        self.directory = args.dataset_dir
        self.type = type
        file_path = os.path.join(self.directory, f"{type}_t_car_price_dataset.pkl")

        # load the data
        with open(file_path, 'rb') as f:
            self.data = pickle.load(f)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx][:-1]
        label = self.data[idx][-1]

        # convert sample to tensor
        input_tensor = torch.tensor(sample.astype(dtype=float), dtype=torch.float32)
        label_tensor = torch.tensor(label.astype(dtype=float), dtype=torch.float32).view(-1)

        # apply the transform
        # input_tensor = create_transform()(input_tensor)

        return input_tensor, label_tensor


def create_dataloader(args: Argument, type: str):
    """
    Create and return the dataloader for the model.
    """
    dataset = CarPriceDataset(args, type)
    return DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
